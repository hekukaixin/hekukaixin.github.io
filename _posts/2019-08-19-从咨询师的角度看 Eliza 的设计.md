---
layout: post
title: 从咨询师的角度看 Eliza 的设计
date: 2018-08-17 15:46
comments: true
external-url:
categories: 思考
---
[*Eliza*](https://store.steampowered.com/app/716500/Eliza/) 是近期发布的一款电子游戏，以人工智能驱动的心理咨询为故事背景。体验之后，我感到它的确有下功夫对于行业进行钻研，且讨论了若干行业可能面临的真实困境。借此机会记录一二。

## 数字化

首先，尽管如游戏之中描述的相对完善的人工智能驱动产品尚且未曾出现，但是，数字化的心理服务最近以来一直是研究热点。市面上已经出现的服务包括：

- [Youper](https://www.youper.ai)
- [Moodnotes](https://moodnotes.thriveport.com)
- [Woebot](https://woebot.io)

知乎问题[“心理咨询师会被人工智能取代吗？”](https://www.zhihu.com/question/55946619)的答案之中列出了更多应用。

数字化有很多好处：便宜（对比心理咨询动辄一次 600 元的费用），简便。同时它的弊端也很明显，比如，脱落率高。我从不排斥这些产品，是因为我认为多元化的心理服务才能最终帮到更多的人。

## 短程疗法与药物治疗

从游戏之中可以看出，*Eliza* 的治疗过程以 15 分钟为一小节，区别于真实的心理咨询多以 45/50 分钟作为一个小节：这可能是为了游戏表现的目的（对比 *In Treatment* 也将小节浓缩至 20 分钟左右，放入一集之中呈现）。但在实际操作之中，15 分钟的限制将会使得触及深刻议题变得困难。

除此之外，*Eliza* 在共情阶段的表现可说良好。但它的治疗取向（明显以短程焦点解决疗法为主）以及药物推荐几乎达到了滥用的程度反应了资本影响下的心理咨询。如果将此处的短程焦点解决疗法替换为认知行为疗法，那么，这就是今天北美心理咨询行业的真实写照了。

## 科技带来的伦理考量

*Eliza* 涉及的一个重要议题即是科技原罪：除了人工智能是否能够替代人类完成心理咨询之外，也包括了摄取个人隐私数据带来的伦理问题。尽管我们目前没有遇到真正相同的问题，但是类似局面的种种考量已经成了一名咨询师的日常困扰。

正如我在[一些工作流](http://hekukaixin.com/总结/2018/07/28/一些工作流/)之中指出的那样，任何涉及“云”的技术都有风险，而讯飞等工具爆出的隐私漏洞也使得一个严格遵守伦理的咨询师无法选择语音文本转换的服务。这些或许距离 *Eliza* 提出的问题尚且遥远，但是，性质实际上是类似的。

## AI VS 人类

关于人类是否一定优于人工智能：有人认为，认知行为疗法或许可以，但是精神分析没有可能。市面上已有的工具似乎也证明了这一点，没有人想要费力不讨好地去做一个精神分析的数字化产品。但是，如果你了解过，早期精神分析试图证明自己是科学的努力，以及分析师通过漫长的个人分析以使自己达成一个空白屏幕的做法，很可能就不会那么确定。顺便说一句，游戏之中，主人公作为人类的干预的确不比人工智能显得专业。

但是，如今的精神分析强调分析师作为“人”的作用，自体心理学，以及由此衍生出的主体间性心理治疗，无不强调主观的重要作用。在这一趋势之下，我会倾向于赞同，人类终究有 AI 无法替代的那个部分。